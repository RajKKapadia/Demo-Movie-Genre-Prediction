{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/logan/Desktop/MyProjects/movie-genre-prediction/project-code/research/cleaned_data/train/cache-b0ac180b1c1e0cb9.arrow\n",
      "Loading cached processed dataset at /home/logan/Desktop/MyProjects/movie-genre-prediction/project-code/research/cleaned_data/test/cache-ae076913fb72cfd6.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 44978,\n",
       " 'movie_name': 'Super Me',\n",
       " 'synopsis': 'A young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon. Selling them makes him rich.',\n",
       " 'genre': 4,\n",
       " 'final_text': 'movie name - super me, synopsis - a young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon. selling them makes him rich..'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_from_disk('cleaned_data/')\n",
    "\n",
    "data = data.class_encode_column('genre')\n",
    "\n",
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distil-bert-base-uncased/ were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distil-bert-base-uncased/ and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "MODEL_PATH = 'distil-bert-base-uncased/'\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_PATH, use_fast=True, do_lower_case=True)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=len(data['train'].features['genre']._int2str),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence len - 100\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for example in data['train']:\n",
    "    input_ids = tokenizer.encode(example['final_text'], add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print(f'Max sentence len - {max_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset:\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.data[item]['final_text'])\n",
    "        target = int(self.data[item]['genre'])\n",
    "        inputs = self.tokenizer(text, max_length=max_len, padding='max_length', truncation=True)\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long).to(device),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long).to(device),\n",
    "            'labels': torch.tensor(target, dtype=torch.long).to(device),\n",
    "        }\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "\n",
    "def train(ds):\n",
    "    ds_train = ds['train']\n",
    "    ds_test = ds['test']\n",
    "\n",
    "    temp_ds = ds_train.train_test_split(test_size=0.1, stratify_by_column='genre')\n",
    "    ds_train = temp_ds['train']\n",
    "    ds_val = temp_ds['test']\n",
    "\n",
    "    train_dataset = ClassificationDataset(ds_train, tokenizer)\n",
    "    valid_dataset = ClassificationDataset(ds_val, tokenizer)\n",
    "    test_dataset = ClassificationDataset(ds_test, tokenizer)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        'result',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    preds = trainer.predict(test_dataset).predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    submission = pd.DataFrame({'id': ds_test['id'], 'genre': preds})\n",
    "    submission.loc[:, 'genre'] = submission.genre.apply(lambda x: ds_train.features['genre'].int2str(x))\n",
    "    submission.to_csv(f'submission_{datetime.now().strftime(\"%d-%m-%Y-%H-%M\")}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/logan/Desktop/MyProjects/movie-genre-prediction/project-code/venv/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17886b81dff4402aa643aa648e91ebbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.939, 'learning_rate': 1.9341672152732063e-05, 'epoch': 0.16}\n",
      "{'loss': 1.7405, 'learning_rate': 1.868334430546412e-05, 'epoch': 0.33}\n",
      "{'loss': 1.7031, 'learning_rate': 1.8025016458196183e-05, 'epoch': 0.49}\n",
      "{'loss': 1.6783, 'learning_rate': 1.7366688610928244e-05, 'epoch': 0.66}\n",
      "{'loss': 1.6769, 'learning_rate': 1.6708360763660305e-05, 'epoch': 0.82}\n",
      "{'loss': 1.6474, 'learning_rate': 1.6050032916392363e-05, 'epoch': 0.99}\n",
      "{'loss': 1.5055, 'learning_rate': 1.5391705069124425e-05, 'epoch': 1.15}\n",
      "{'loss': 1.5075, 'learning_rate': 1.4733377221856486e-05, 'epoch': 1.32}\n",
      "{'loss': 1.4942, 'learning_rate': 1.4075049374588544e-05, 'epoch': 1.48}\n",
      "{'loss': 1.5152, 'learning_rate': 1.3416721527320606e-05, 'epoch': 1.65}\n",
      "{'loss': 1.5004, 'learning_rate': 1.2758393680052667e-05, 'epoch': 1.81}\n",
      "{'loss': 1.5037, 'learning_rate': 1.2100065832784729e-05, 'epoch': 1.97}\n",
      "{'loss': 1.3629, 'learning_rate': 1.1441737985516787e-05, 'epoch': 2.14}\n",
      "{'loss': 1.3243, 'learning_rate': 1.0783410138248848e-05, 'epoch': 2.3}\n",
      "{'loss': 1.3376, 'learning_rate': 1.012508229098091e-05, 'epoch': 2.47}\n",
      "{'loss': 1.337, 'learning_rate': 9.46675444371297e-06, 'epoch': 2.63}\n",
      "{'loss': 1.3326, 'learning_rate': 8.808426596445029e-06, 'epoch': 2.8}\n",
      "{'loss': 1.3387, 'learning_rate': 8.15009874917709e-06, 'epoch': 2.96}\n",
      "{'loss': 1.2237, 'learning_rate': 7.491770901909151e-06, 'epoch': 3.13}\n",
      "{'loss': 1.1898, 'learning_rate': 6.833443054641213e-06, 'epoch': 3.29}\n",
      "{'loss': 1.1654, 'learning_rate': 6.175115207373272e-06, 'epoch': 3.46}\n",
      "{'loss': 1.1828, 'learning_rate': 5.516787360105334e-06, 'epoch': 3.62}\n",
      "{'loss': 1.183, 'learning_rate': 4.8584595128373935e-06, 'epoch': 3.79}\n",
      "{'loss': 1.1847, 'learning_rate': 4.200131665569454e-06, 'epoch': 3.95}\n",
      "{'loss': 1.1034, 'learning_rate': 3.5418038183015147e-06, 'epoch': 4.11}\n",
      "{'loss': 1.0638, 'learning_rate': 2.883475971033575e-06, 'epoch': 4.28}\n",
      "{'loss': 1.0664, 'learning_rate': 2.2251481237656355e-06, 'epoch': 4.44}\n",
      "{'loss': 1.0682, 'learning_rate': 1.5668202764976959e-06, 'epoch': 4.61}\n",
      "{'loss': 1.0713, 'learning_rate': 9.084924292297564e-07, 'epoch': 4.77}\n",
      "{'loss': 1.0659, 'learning_rate': 2.50164581961817e-07, 'epoch': 4.94}\n",
      "{'train_runtime': 7435.7961, 'train_samples_per_second': 32.68, 'train_steps_per_second': 2.043, 'train_loss': 1.363499168411064, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a94cb9af4e4d70af6fed5a7dc7865c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
